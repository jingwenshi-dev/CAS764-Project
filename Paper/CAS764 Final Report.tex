\documentclass[sigconf, nonacm]{acmart}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}

\usepackage{mdframed}

% Define the style for example blocks
\definecolor{examplecolor}{rgb}{1,1,1}  % White background
\definecolor{exampleborder}{rgb}{0.8,0,0}  % Red border color

% Create custom example environment
\newenvironment{example}
{\begin{mdframed}[
    linecolor=exampleborder,
    linewidth=0.5pt,
    backgroundcolor=examplecolor,
    topline=false,
    bottomline=false,
    leftline=true,
    rightline=false,
    innertopmargin=2pt,
    innerbottommargin=2pt,
    innerleftmargin=5pt,
    innerrightmargin=5pt,
    skipabove=10pt,
    skipbelow=10pt
]}
{\end{mdframed}}

\theoremstyle{definition}
\newtheorem{exmp}{Example}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{delim}{RGB}{20,105,176}
\definecolor{numb}{RGB}{106, 109, 32}
\definecolor{string}{rgb}{0.64,0.08,0.08}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
}

\lstdefinelanguage{json}{
    showspaces=false,
    showtabs=false,
    breaklines=true,
    postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{gray}\hookrightarrow\space}},
    breakatwhitespace=true,
    basicstyle=\ttfamily\small,
    upquote=true,
    morestring=[b]",
    stringstyle=\color{string},
    escapeinside={(*@}{@*)},  % Add this line
    texcl=false,              % Add this line
    mathescape=false,         % Add this line
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1}
      {:}{{{\color{delim}{:}}}}{1}
      {,}{{{\color{delim}{,}}}}{1}
}

\lstset{style=mystyle}

\AtBeginDocument{\providecommand\BibTeX{{Bib\TeX}}}

\begin{document}

\title{Deceptive Review Detection with LLMs}

\author{Jingwen Shi}
\email{shij67@mcmaster.ca}
\affiliation{
  \institution{McMaster University}
  \city{Hamilton}
  \state{Ontario}
  \country{Canada}
}

\renewcommand{\shortauthors}{Jingwen Shi}

\begin{abstract}
Detecting deceptive reviews has become a critical challenge as fake reviews continue to increase, influencing consumer trust and purchasing decisions. This study evaluates the effectiveness of large language models (LLMs) in identifying deceptive reviews. Three datasets are used with few-shot testing to assess the performance of LLMs in learning from examples across different domains and languages. In general, LLMs demonstrate the ability to adapt knowledge from in-context examples to some extent, but their performance depends on the language of the training set and the complexity of the test cases' sentiment patterns. The findings highlight the potential of LLMs in automated fake review detection, reducing the reliance on training machine learning models from scratch.
\end{abstract}

\maketitle

\section{Introduction}
Deceptive reviews are a form of opinion spamming. These reviews are intentionally misleading and aim to influence a product's rating, either positively or negatively, by posting false opinions to promote or discredit target products, services, or individuals.

\subsection{Importance of Deceptive Review Detection}
The concern regarding such activities has steadily grown due to the development of automated bots, also known as spambots, is designed for such purpose. As a result, a significant percentage of reviews on the internet are fake, which can mislead consumers and impact people's purchasing decisions.

Those deceptive reviews could rise to the top of review platforms, drowning out the truthful ones. Businesses that benefit from fake reviews may gain an unfair market advantage, which can lead to a decrease in the availability of high-quality products on the market. This phenomenon is also referred to as Gresham's Law, where "bad money drives out good."

\subsection{Challenges}
Deceptive reviews often look similar to the truthful ones. Manually verifying them is not only tedious and time-consuming but also challenging, as deception can occur in many ways, including exaggeration, downplaying flaws, or providing misleading information.

Traditional methods for detecting such reviews often involve sentiment analysis using feature-based approaches or natural language processing. However, training these models can be expensive and time-consuming. Also, currently, there is no well-labeled dataset of deceptive reviews with broad population and domain coverage. Therefore, it will be a technical challenge to train a machine-learning model from the beginning.

\subsection{Objective}
The objective of our study is to explore the feasibility of using Large Language Models (LLMs) to detect deceptive reviews. This approach is based on the assumption that LLMs are pre-trained on extensive corpora, which has already included deceptive reviews. Therefore, LLMs should have the capability to understand the context of the study and identify deceptive reviews to some extent.

Note that, in this study, deceptive reviews are defined as those that are human-written and intentionally misleading. This definition excludes reviews generated by LLMs or other automated systems.

\subsection{Findings}
Using three datasets from different domains and languages, this study evaluates the performance of LLMs in zero-shot, 20-shot, and 40-shot testing by comparing their outputs to ground truth labels.

\noindent The major findinds are as follows:
\begin{itemize}
  \item[1.] LLMs show a slight improvement in accuracy when more in-context examples are provided.
  \item[2.] LLMs perform better at detecting positive reviews than negative reviews.
  \item[3.] When given English in-context hotel review examples, LLMs show significant improvements in precision and recall for Arabic testset.
  \item[4.] When given Arabic in-context hotel review examples, LLMs show no improvement in English testset.
  \item[5.] With app reivew dataset, point 3 is also observed. However, the results are opposite to observation 4.
\end{itemize}

The remainder of this paper is organized as follows: Section 2 reviews related work on deceptive review and fake news detection. Section 3 describes the methodology, including dataset preparation and evaluation metrics. Section 4 discusses potential applications. Section 5 presents the results and analysis. Section 6 draws conclusions. Finally, Section 7 outlines limitations and suggests future directions.

\section{Past Work}
The number of studies directly addressing the detection of human-written deceptive reviews is limited. However, there has been significant research on fake news detection and the detection of LLM-generated text.

One of the most widely used methods for detecting fake news, deceptive reviews, and similar tasks is to train or fine-tune machine learning models on large corpora of text data. While the specific models and datasets may vary, the general approach is to train a model to learn the patterns of deceptive reviews and find out the best one. For instance, some studies use transformer-based models such as BERT, while others use classifiers or alternative architectures. Additionally, some studies focus on plain text data, while others have more diverse data structures, such as graphs \cite{liyanage-etal-2024-detecting, ott-etal-2011-finding, SALMINEN2022102771, ignat2024maideupmultilingualdeceptiondetection, 8259828}.

Another approach is analyzing linguistic features by comparing fake reviews with truthful ones. These studies focus on syntactic and lexical elements which rely on textual features to distinguish deceptive content \cite{ignat2024maideupmultilingualdeceptiondetection, 8259828, abri2020fakereviewsdetectionanalysis}.

Furthermore, some studies propose innovative methods to detect fake reviews. For example, one study developed an Adaptive Rationale Guidance Network, which integrates insights from LLM-generated rationales into small language models (SLMs) to enhance fake news detection performance \cite{Hu_2024}.

However, most existing studies primarily focus on either fake news detection or the identification of AI-generated text. The majority of these works either train models or analyze linguistic features to detect deceptive reviews. There is a limited number of studies focusing on the direct detection of human-written deceptive reviews using LLMs with few-shot learning. This study aims to address this gap by exploring the feasibility of using LLMs to detect deceptive reviews.

\section{Methodology}

\subsection{Datasets}
Three datasets from Kaggle are used in this study to evaluate the performance of LLMs in deceptive review detection. The datasets are as follows:

\begin{itemize}
  \item[1.] \href{https://www.kaggle.com/datasets/rtatman/deceptive-opinion-spam-corpus}{Deceptive Opinion Spam Corpus} \cite{ott-etal-2011-finding, ott-etal-2013-negative}
  \item[2.] \href{https://www.kaggle.com/datasets/shathaalturke/afrd-arabic-fake-reviews-detection}{AFRD\_Arabic-Fake-Reviews-Detection}
  \item[3.] \href{https://www.kaggle.com/datasets/umairanjacks/fake-reviews-of-apps}{Fake Reviews of apps}
\end{itemize}

The Deceptive Opinion Spam Corpus Dataset (hereafter referred to as the Baseline or Hotel Review Dataset) is used as the baseline dataset. This dataset is well-balanced and consisting of a total of 1600 rows data which evenly distributed across four categories (see Table 1). Each category contains 20 reviews for each of the 20 most popular hotels in Chicago. The large and balanced scale of this dataset makes it a good representation of the population, making it ideal for use as the baseline dataset.

\begin{table}[h!]
  \centering
  \caption{Distribution of reviews for Hotel Review Dataset}
  \begin{tabular}{c c c}
    \toprule
              & Positive & Negative \\
    \midrule
    Truthful  & 400      & 400      \\
    Deceptive & 400      & 400      \\
    \bottomrule
  \end{tabular}
\end{table}

The AFRD\_Arabic-Fake-Reviews-Detection Dataset (hereafter referred to as the Arabic Dataset) is used to evaluate the performance of LLMs across different languages and domains (see Table 2). This dataset contains three domains: hotel, restaurant, and product. Although the dataset is not balanced across these domains, it includes sufficient data rows across veracity, which is the most important factor for this study.

\begin{table}[h!]
  \centering
  \caption{Distribution of reviews for Arabic Dataset}
  \begin{tabular}{c c c c}
    \toprule
                 & Truthful & Deceptive & Total \\
    \midrule
    Hotel        & 155      & 155       & 310   \\
    Restaurant   & 357      & 357       & 714   \\
    Product      & 352      & 352       & 704   \\
    Multi-domain & 864      & 864       & 1728  \\
    \bottomrule
  \end{tabular}
\end{table}

Finally, the Fake Reviews of Apps Dataset (hereafter referred to as the App Review Dataset) is used to compare the performance of LLMs with both the hotel review dataset and the Arabic dataset. While this dataset is unbalanced in terms of veracity and lacks labels on polarity, it contains enough data rows to be used in this study (see Table 3).

\begin{table}[h!]
  \centering
  \caption{Distribution of reviews for App Review Dataset}
  \begin{tabular}{c c c}
    \toprule
     & Truthful & Deceptive \\
    \midrule
     & 4194     & 1544      \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Evaluation Metrics}
Accuracy, precision, and recall are used as evaluation metrics in this study:

\begin{itemize}
  \item \textbf{Accuracy} is evaluated as the percentage of correct classifications (both positive and negative) out of the total number of samples. It provides an overall measure of the model's performance across all classes.
        \[
          \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
        \]
  \item \textbf{Precision} indicates how many of the reviews classified as deceptive are truly deceptive. It focuses on minimizing false positives, which are cases where truthful reviews are incorrectly classified as deceptive.
        \[
          \text{Precision} = \frac{TP}{TP + FP}
        \]
  \item \textbf{Recall} (also known as sensitivity) reflects the percentage of actual deceptive cases that were correctly identified by the model. It aims to minimize false negatives, where deceptive reviews are mistakenly classified as truthful.
        \[
          \text{Recall} = \frac{TP}{TP + FN}
        \]
\end{itemize}

\noindent The definitions of the terms used in the formulas are as follows:
\begin{itemize}
  \item \textbf{True Positives (TP)}: Deceptive reviews correctly classified as deceptive.
  \item \textbf{True Negatives (TN)}: Truthful reviews correctly classified as truthful.
  \item \textbf{False Positives (FP)}: Truthful reviews incorrectly classified as deceptive.
  \item \textbf{False Negatives (FN)}: Deceptive reviews incorrectly classified as truthful.
\end{itemize}

\subsection{Sampling}
For the Hotel Review Dataset and the Arabic Dataset, both datasets will be split into four strata: truthful positive reviews, truthful negative reviews, deceptive positive reviews, and deceptive negative reviews. A fixed random seed will be used to randomly select 0, 5, and 10 reviews from each stratum, following a stratified sampling approach. The selected reviews will serve as in-context examples for zero-shot, 20-shot, and 40-shot testing. Finally, 10 rows of data will be randomly selected from each stratum to create the test set.

Similarly, the App Review Dataset will be divided into two strata: truthful reviews and deceptive reviews, as it does not include polarity information. From this dataset, 0, 10, and 20 reviews will be randomly selected for the training set, and 20 reviews will be randomly selected for the test set to match the number of reviews selected from the other two datasets.

\subsection{Experiments}

Zero-shot and few-shot testing will be performed on the Hotel Review Dataset as the baseline. The LLM will receive either 0, 5, or 10 in-context examples from each stratum.

Next, a polarity test will be conducted to evaluate whether sentiment polarity influences the LLM's ability to detect veracity. The in-context examples will remain unchanged, while the test set will consist only either positive or negative reviews.

For multilingual testing, either the training set or test set will be replaced with data from the Arabic Dataset to examine the LLM's ability to transfer knowledge from one language to another.

Finally, the above process will be repeated on the App Review Dataset to compare the performance of LLMs across different domains. However, the polarity test will be omitted, as polarity information is not available in the App Review Dataset.

\subsection{API Paraneters \& Prompt Engineering}

The LLM used in this study is the gpt-4o-2024-08-06 model from OpenAI with a default temperature of 0.7. A lower temperature is not utilized because it causes the LLM's responses to remain consistent across different runs, which loses the point of calculating average results from multiple experiments. In addition, using a low temperature (e.g., 0.1) somehow results in an accuracy of 50\% regardless of the number of in-context examples provided. Therefore, the default temperature seems to be a decent choice for this study.

The initial response format (see Listing 1) is a list of strings, where each string is either "deceptive" or "truthful." This response format may vary depending on the specific prompt used, which will be discussed later.

\begin{lstlisting}[language=Python, caption={Initial Response Format}]
class Result(BaseModel):
    results: list[str]
\end{lstlisting}

Two types of messages are sent to the LLMs: system messages and user messages (see Listing 2). The same prompt is used throughout the entire study to ensure fairness in comparison. The prompt is fed to the LLMs with API calls, and the results returned are compared against the ground truth.

\begin{lstlisting}[language=json, caption={Initial Prompt Messages}]
{
    "role": "system",
    "content": (
        "You are an expert in identifying deceptive reviews."
        "Deceptive review samples: "
        "Example review 1 ..."
        "Truthful review samples: "
        "Example review 2 ..."
    )
}
{
    "role": "user",
    "content": (
        "Now classify the following reviews as either 'truthful' or 'deceptive': "
        "Test review 1 ..."
        "Test review 2 ..."
    )
}
\end{lstlisting}

However, the initial prompt was insufficient to receive the expected response format from ChatGPT-4o. The first problem encountered was that the API returned a list containing only one string (i.e. ['truthful'] or ['deceptive']). By emphasizing that the LLM should classify each review individually (see Listing 3), the API was able to correctly return a list of strings instead of a single string.

\begin{lstlisting}[language=json, caption={Updated User Messages 1}]
{
  "role": "user",
  "content": (
  "Now classify (*@\textbf{each of the}@*) following reviews as either 'truthful' or 'deceptive'. "
  "..."
  )
}
\end{lstlisting}

After updating the prompt, a second issue arose, that is, AI hallucination. The LLM randomly returned more outputs than the number of test items provided. By adding a constraint specifying that the results should not exceed the number of test items (see Lisiting 4), this issue was resolved.

\begin{lstlisting}[language=json, caption={Updated User Messages 2}]
{
  "role": "user",
  "content": (
      "Now classify each of the following (*@\textbf{\{total\_inputs\}}@*) reviews as either 'truthful' or 'deceptive'. "
      "The results should not contain more than (*@\textbf{\{total\_inputs\}}@*) items."
      "..."
  )
}
\end{lstlisting}

Then the opposite problem occurred: the response from the LLM was truncated. This results in fewer classifications than the number of input test items. To solve this issue, the index of each row in the test data was added to the prompt followed by the corresponding test review. The response formate was also modifed to require the LLM to return both the index of the original test review and its prediction (See Listing 5 \& 6). This ensured that the LLM returned the exact number of predictions matching the number of test items.

\begin{lstlisting}[language=json, caption={Updated User Messages 3}]
{
  "role": "user",
  "content": (
      "Now classify each of the following {total_inputs} reviews as either 
      'truthful' or 'deceptive'. "
      "The results should not contain more than {total_inputs} items."
      (*@\textbf{"{test\_set['index']}: {test\_set['review']}"}@*)
      "..."
  )
}
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Final Response Format}]
class PredictionItem(BaseModel):
    index: int
    prediction: str

class Result(BaseModel):
  results: list[PredictionItem]
\end{lstlisting}

Finally, the prompt is futher refined to include updated in-context example formats and response formats (see Listing 7). This resolved the issue where the list of prediction indices returned by the LLM began at 0 instead of the original indices provided.

\begin{lstlisting}[language=json, caption={Final User Messages}]
{
  "role": "user",
  "content": (
      "Now classify each of the following {total_inputs} reviews as either 'truthful' or 'deceptive'. "
      "The results should not contain more than {total_inputs} items."
      (*@\textbf{"The reviews are given as an index and text pair, e.g. '123: This is a review.'"}@*)
      (*@\textbf{"Return your response as a JSON list of classifications paired with an index, e.g. [{'123': 'truthful'}, {'456': 'deceptive'}, ...]."}@*)
      "{test_set['index']}: {test_set['product_review']}"
      "..."
  )
}
\end{lstlisting}

\section{Potential Applications}

This approach offers small companies a cost-effective solution for integrating deceptive review detection into their systems. Businesses that lack the resources to train their own machine-learning models could use this method to identify and remove fake reviews effectively.

Also, since LLMs are publicly accessible as third-party tools, individuals can use them to verify the credibility of reviews when they doubt the truthfulness of reviews. Such accessibility could prevent individuals or organizations from manipulating product reviews for personal gain, which leads to a more transparent marketplace.

By reducing the prevalence of deceptive reviews, this approach helps protect consumer rights and minimizes the risk of fraud in online transactions.

\section{Results \& Analysis}
In this section, the results of the experiments in the order introduced in Section 3.4, along with statistical analyses and speculations on the findings.

Note that, Sections 5.1 to 5.3 utilize the Baseline Dataset, while Sections 5.4 and 5.5 focus on the App Review Dataset to enable comparisons with the Baseline Dataset. Secion 5.3 can only perform on the Baseline Dataset as the App Review dataset does not include polarity information.

\subsection{Baseline}
In the graph of baseline results (see Figure 1), GPT-4o shows an improvement in accuracy to some extent as more in-context examples are provided. Similarly, the increasing precision with the number of in-context examples highlights GPT-4o's ability to minimize false positives as it learns from more examples. However, recall remains consistent across different numbers of in-context examples. This suggests that the model may become optimized for one class after training on examples but at the expense of performance in the other class.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{diagrams/baseline.png}
  \caption{Baseline Results}
\end{figure}

\subsection{Polarity Test with Baseline Dataset}
The difference between polarity tests and baseline tests is that polarity labels are included in the in-context examples provided in the prompt, and all test reviews share the same polarity (see Listing 8).

For the remainder of this section, when all test reviews are positive, it will be referred to as a \textbf{positive polarity test}. Similarly, when all test reviews are negative, it will be referred to as a \textbf{negative polarity test}.

\begin{lstlisting}[language=json, caption={System Messages for Polarity Test}]
{
  "role": "system",
  "content": (
      "You are an expert in identifying deceptive reviews."
      "Deceptive (*@\textbf{positive}@*) review samples: "
      "Example review 1 ..."
      "Deceptive (*@\textbf{negative}@*) review samples: "
      "Example review 2 ..."
      "Truthful (*@\textbf{positive}@*) review samples: "
      "Example review 3 ..."
      "Truthful (*@\textbf{negative}@*) review samples: "
      "Example review 4 ..."
  )
}
\end{lstlisting}

The results indicate that both detection accuracy and recall for positive reviews eventually improve, while precision remains unchanged at the end (see Figure 2). Although the reason for the initial 10\% improvement in precision followed by a drop to the original level remains unclear, all three metrics show improvements of 6.8\%, 9.6\%, and 4\%, respectively, compared to the baseline results at 40 shots.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{diagrams/polarity_positive_metrics.png}
  \caption{Positive Polarity Results}
\end{figure}

For the negative polarity test (see Figure 3), although all three metrics show some improvement at 40 shots compared to 0 shots, the results highlight some anomalies. Accuracy increases by only around 3\%, and recall decreases by 15\% compared to the baseline results at 40 shots. On the other hand, precision shows an opposite trend to the positive polarity test, it drops by 10\% before dramatically increasing by 60\% at 40 shots.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{diagrams/polarity_negative_metrics.png}
  \caption{Negative Polarity Results}
\end{figure}

While the recall for the negative polarity test is much higher than for the positive polarity test at 40 shots, in all other cases, the metrics for the negative polarity test are significantly lower than those for the positive polarity test. Therefore, it iss afe to say that, on average, the positive polarity test outperforms the negative polarity test, regardless of the number of in-context examples provided.

A possible explanation for the LLM's better performance on positive fake reviews is that such reviews often have exaggerated sentiment patterns that are easier to detect. In contrast, negative fake reviews tend to use subtler language, which is harder to detect.

\subsection{Multilingual Test with Arabic Dataset}
In this section, the Arabic Dataset is used either as the test set (referred to as the English-to-Arabic test) or the training set (referred to as the Arabic-to-English test) to evaluate the LLM's ability to transfer and apply knowledge from one language to another.

Tables are provided as the stacked numbers in the graphs can be difficult to read. The tables show the accuracy, precision, and recall of the LLMs across different numbers of in-context examples.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{diagrams/eng_arb_metrics.png}
  \caption{English to Arabic Results}
\end{figure}

For English to Arabic test (see Figure 4 \& Table 4), while the accuracy remains consistent at approximately 55\% regardless of the number of in-context examples, precision and recall show some improvement. This indicates that the model becomes better at correctly identifying fake reviews and minimizing false positives while improving its ability to identify all actual deceptive reviews.

However, for Arabic to English test (see Figure 5 \& Table 4), there is nearly no change in all three metrics across different numbers of in-context examples. This suggests that the LLMs are unable to effectively learn knowledge from Arabic and apply it to English.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{diagrams/arb_eng_metrics.png}
  \caption{Arabic to English Results}
\end{figure}

\begin{table}[h!]
  \centering
  \caption{Eng-Arb and Arb-Eng Metrics by Shot Count}
  \begin{tabular}{c c c c}
    \toprule
    \textbf{Metric} & \textbf{Zero Shot (\%)} & \textbf{20 Shots (\%)} & \textbf{40 Shots (\%)} \\
    \midrule
    \multicolumn{4}{c}{\textbf{Eng-Arb}}                                                        \\
    \midrule
    Accuracy        & 52.00                   & 55.00                  & 56.80                  \\
    Precision       & 22.80                   & 50.20                  & 59.00                  \\
    Recall          & 16.00                   & 39.00                  & 47.00                  \\
    \midrule
    \multicolumn{4}{c}{\textbf{Arb-Eng}}                                                        \\
    \midrule
    Accuracy        & 51.80                   & 50.00                  & 52.20                  \\
    Precision       & 51.80                   & 50.00                  & 51.80                  \\
    Recall          & 52.00                   & 50.00                  & 54.00                  \\
    \bottomrule
  \end{tabular}
\end{table}

One possible explanation for such an observation is that LLMs are predominantly trained on high-quality English data while other languages being less represented in the training data. Therefore, the model may have difficulty understanding Arabic in-context examples. This lower its ability to transfer and apply knowledge from Arabic to English contexts effectively.

\subsection{Few Shot Testing with App Review Dataset}

If the baseline experiment is rerun using the App Review Dataset, the results suggest that the ability of LLMs to learn from in-context examples has been underestimated. It is clear to see that initial accuracy and precision are significantly higher compared to the baseline experiment with the Hotel Review Dataset. Furthermore, all three metrics surpass 96\% by 20 shots, with no significant improvement observed at 40 shots, which is understandable (see Figure 6).

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{diagrams/baseline_app.png}
  \caption{Few Shot Testing Results with App Review Dataset}
\end{figure}

\subsection{Multilingual Test with App Review Dataset}

For the English-to-Arabic test (see Figure 7 \& Table 5), the trends for all three metrics across the number of shots are the same with those observed in the multilingual test using the Baseline Dataset.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{diagrams/eng_arb_metrics_app.png}
  \caption{English to Arabic Results with App Review Dataset}
\end{figure}

However, for the Arabic-to-English test (see Figure 8 \& Table 5), precision maintains at a level comparable to the App Review baseline, while accuracy and recall improve as more in-context examples are provided, reaching approximately 94\%.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{diagrams/arb_eng_metrics_app.png}
  \caption{Arabic to English Results with App Review Dataset}
\end{figure}

\begin{table}[h!]
  \centering
  \caption{Eng-Arb and Arb-Eng Metrics by Shot Count (w\ App Review Dataset)}
  \begin{tabular}{c c c c}
    \toprule
    \textbf{Metric} & \textbf{Zero Shot (\%)} & \textbf{20 Shots (\%)} & \textbf{40 Shots (\%)} \\
    \midrule
    \multicolumn{4}{c}{\textbf{Eng-Arb}}                                                        \\
    \midrule
    Accuracy        & 51.00                   & 55.40                  & 57.20                  \\
    Precision       & 21.20                   & 56.60                  & 59.00                  \\
    Recall          & 20.00                   & 50.00                  & 48.40                  \\
    \midrule
    \multicolumn{4}{c}{\textbf{Arb-Eng}}                                                        \\
    \midrule
    Accuracy        & 62.20                   & 70.80                  & 94.00                  \\
    Precision       & 96.60                   & 88.00                  & 94.60                  \\
    Recall          & 26.00                   & 49.00                  & 94.00                  \\
    \bottomrule
  \end{tabular}
\end{table}

This shows that the LLM is indeed capable of learning from Arabic examples and contradicts the previous speculation that LLMs are unable to transfer knowledge from Arabic and applies to English.

The observations in Sections 4.4 and 4.5 suggest two possible explanations for this phenomenon:
\begin{enumerate}
  \item The App Review Dataset has clearer patterns for the LLM to learn from compared to the Hotel Review Dataset.
  \item The App Review Dataset may have been exposed to LLMs during the training process, which makes the model have a better understanding of the context of the dataset.
\end{enumerate}

The first speculation is more plausible and will be further explored in the next section (see Section 5.6) while the second one is unfalsifiable.

\subsection{Dataset Impact on Performance}

Below are examples from the App Review Dataset. It is clear that the deceptive reviews tend to be much shorter, often starting in the first person and ending with an exclamation mark. Compared to truthful reviews, deceptive reviews generally lack detail and are vague. In contrast, truthful reviews have more complex language patterns and may even include misspellings.

\begin{example}
  \textbf{Deceptive Example:}
  \begin{enumerate}
    \item I couldn’t believe my eyes when I saw how great this app is!
    \item "I told all my colleagues about this app, and they are all using it now!"
    \item "I organized all my tasks with this app, and it even helped me prioritize which tasks were the most important!"
  \end{enumerate}
\end{example}

\begin{example}
  \textbf{Tuthful Example:}
  \begin{enumerate}
    \item This is a waste app. After few days it started to shiw nothing and it says it takes time to sync with the calender but still after 24 hrs they dont sync. Please dont install this app and then expect to be productive.
    \item "The app’s file backup system is reliable, but it doesn’t support real-time syncing."
    \item "I simply love TickTick but am growing weary of the widgets not updating. If I open the app, they update after a minute or so. However, if I don't need to use the app (therefore not opening it), the widgets stop updating after some amount of time. Is there a setting I'm missing, or do I need to ""not optimize"" TickTick's battery use? EDIT: In addition, actions taken via the widget do not update the app/website until i open the app explicitly."
  \end{enumerate}
\end{example}

However, the Hotel Review Dataset (i.e., Baseline Dataset) does not exhibit such clear patterns (see examples below). The deceptive reviews are not necessarily shorter than the truthful reviews, and the language patterns are less distinct compared to the App Review Dataset. Both deceptive reviews and truthful reviews contain a lot of details such that it is difficult for those unfamiliar with the location to identify if the details are real. Therefore, LLMs face greater difficulty in detecting deceptive reviews within the Hotel Review Dataset compared to the App Review Dataset.

\begin{example}
  \textbf{Deceptive Example:}
  \begin{enumerate}
    \item My husband and I visited the Fairmont Chicago Millennium Park for our honeymoon. The customer service was amazing. From the time we booked our packege to the time we checked in everything was absolutely amazing. These people were proficient, respectful and very thoughtful. The Fairmont had a lounge, a wine room, a bar and a restaurant. I couldn't decide where I wanted to go first! After we put our bags up we headed down to the wine room... It was totally delicious. We also got free wine just because it was our honeymoon! Then after a few glasses of wine we hit the spa, once again excellent!! Everything smells like honeysuckle and everyone smiles all the time. We stayed in the gold room. Although it was a little bit smaller than I thought it would be I was definitely satisfied with the huge king bed with even bigger pillows. My husband and I relaxed in fluffy white bath robes while we sipped champagne while we watched the sparkling lights of the city. It was a wonderful experience that I will never forget. Four thumbs up!
  \end{enumerate}
\end{example}

\begin{example}
  \textbf{Tuthful Example:}
  \begin{enumerate}
    \item We stayed for a one night getaway with family on a thursday. Triple AAA rate of 173 was a steal. 7th floor room complete with 44in plasma TV bose stereo, voss and evian water, and gorgeous bathroom(no tub but was fine for us) Concierge was very helpful. You cannot beat this location... Only flaw was breakfast was pricey and service was very very slow(2hours for four kids and four adults on a friday morning) even though there were only two other tables in the restaurant. Food was very good so it was worth the wait. I would return in a heartbeat. A gem in chicago...
  \end{enumerate}
\end{example}

\section{Conclusions}

In conclusion, the experimental results show that LLMs have a slight improvement in accuracy when more in-context examples are given. However, the model performs better at detecting positive reviews than negative ones. A potential reason for this is that people tend to include more details in negative deceptive reviews than in positive ones. Further investigation and more robust evidence are needed to support this speculation.

When given English in-context hotel review examples, LLMs show significant improvements in precision and recall for Arabic datasets, but not in reverse. The speculation is that LLMs being trained predominantly on high-quality English data, making it challenging for the model to fully understand Arabic in-context examples.

However, with the App Review Dataset, the LLMs demonstrate the ability to learn knowledge from Arabic examples and apply it to English test cases. This suggests that the App Review Dataset may include easier patterns for the LLMs to learn from compared to the Hotel Review Dataset.

\section{Limitations \& Future Directions}
There are several limitations in this study. At the current stage, if more than 40 in-context examples and 40 test reviews are included in a prompt, it may exceed the token limitations of GPT-4o, results in multiple API calls. However, this study lacks funding, making it challenging to send multiple API calls. This limitation is also the reason why each experiment was repeated only five times due to budget constraints.

In the future, the robustness and reliability of the results can be improved as more runs are performed. Additionally, the study could be extended to include other LLMs to determine whether the findings are consistent across different models.

The significant improvement in LLM performance on a different dataset also needs to be further investigated. Future research could explore whether the LLM was exposed to similar language patterns during its training process. It would also be valuable to find a way to quantify the effectiveness of in-context examples derived from both the Hotel and App Review Datasets.

Finally, it is necessary to examine whether the Arabic Dataset or the Baseline Dataset contributed to the poor multilingual performance of the LLM and investigate how different languages impact the model's performance. Understanding these factors could provide insights into optimizing the performance of LLMs for multilingual tasks.

\bibliographystyle{ACM-Reference-Format}
\bibliography{final-paper}

\end{document}
\endinput
